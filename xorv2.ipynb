{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.476\n",
      "step: 200, loss: 0.034\n",
      "step: 400, loss: 0.019\n",
      "step: 600, loss: 0.017\n",
      "step: 800, loss: 0.015\n",
      "X: array([[0, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 1]])\n",
      "pred: array([[0.21001229],\n",
      "       [0.9883025 ],\n",
      "       [0.9885254 ],\n",
      "       [0.10054824]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# %load src/XORV2.py\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "   file name: XORV2.py\n",
    "   create time: Sun 16 Jul 2017 07:00:20 AM EDT\n",
    "   author: Jipeng Huang\n",
    "   e-mail: huangjipengnju@gmail.com\n",
    "   github: https://github.com/hjptriplebee\n",
    "   function: improved model can fit XOR\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1_1 = tf.Variable(tf.random_normal([2, 1]))\n",
    "w1_2 = tf.Variable(tf.random_normal([2, 1]))\n",
    "w2 = tf.Variable(tf.random_normal([2, 1]))\n",
    "\n",
    "b1_1 = tf.constant(0.1, shape=[1])\n",
    "b1_2 = tf.constant(0.1, shape=[1])\n",
    "b2 = tf.constant(0.1, shape=[1])\n",
    "\n",
    "h1 = tf.nn.relu(tf.matmul(x, w1_1) + b1_1)\n",
    "h2 = tf.nn.relu(tf.matmul(x, w1_2) + b1_2)\n",
    "\n",
    "hidden = tf.concat([h1, h2], 1)\n",
    "out = tf.matmul(hidden, w2) + b2\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(out - y))\n",
    "\n",
    "train = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        sess.run(train, feed_dict={x: X, y: Y})\n",
    "        loss_ = sess.run(loss, feed_dict={x: X, y: Y})\n",
    "        if not i%200:\n",
    "            print(\"step: %d, loss: %.3f\"%(i, loss_))\n",
    "    print(\"X: %r\"%X)\n",
    "    print(\"pred: %r\"%sess.run(out, feed_dict={x: X}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: <tf.Tensor 'Const_50:0' shape=(4,) dtype=float32>\n",
      "Y: <tf.Tensor 'my_st_tf:0' shape=<unknown> dtype=float32>\n",
      "[-0.3    0.005  0.08   0.12 ]\n",
      "[0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#自定义激活函数\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "def my_stack_def(x, threshold=0.0):\n",
    "    if x>threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "my_stack_def = np.vectorize(my_stack_def)\n",
    "my_stack_def_32 = lambda x: my_stack_def(x).astype(np.float32)\n",
    "\n",
    "\n",
    "def my_stack_tf(x, name=None):\n",
    "    with ops.name_scope(name, \"my_stack_tf\", [x]) as name:\n",
    "        y = tf.py_func(my_stack_def_32,\n",
    "                       [x],\n",
    "                       [tf.float32],\n",
    "                       name=name,\n",
    "                       stateful=False)\n",
    "        return y[0]\n",
    "    \n",
    "def my_py_func(func, inp, Tout, stateful=False, name=None, my_grad_func=None):\n",
    "    # Need to generate a unique name to avoid duplicates:\n",
    "    random_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "    tf.RegisterGradient(random_name)(my_grad_func)  # see _my_relu_grad for grad example\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({\"PyFunc\": random_name, \"PyFuncStateless\": random_name}):\n",
    "        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n",
    "    \n",
    "def _my_stack_grad(op, pre_grad):\n",
    "    x = op.inputs[0]\n",
    "    cur_grad = my_stack_tf(x)\n",
    "    next_grad = pre_grad * cur_grad\n",
    "    return next_grad\n",
    " \n",
    "def my_st_tf(x, name=None):\n",
    "    with ops.name_scope(name, \"my_st_tf\", [x]) as name:\n",
    "        y = my_py_func(my_stack_def_32,\n",
    "                       [x],\n",
    "                       [tf.float32],\n",
    "                       stateful=False,\n",
    "                       name=name,\n",
    "                       my_grad_func=_my_stack_grad)  # <-- here's the call to the gradient\n",
    "        return y[0]\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    x = tf.constant([-0.3, 0.005, 0.08, 0.12])\n",
    "    y = my_st_tf(x)\n",
    "    #tf.global_variables_initializer().run()\n",
    "    print(\"X: %r\"%x)\n",
    "    print(\"Y: %r\"%y)\n",
    "    print(x.eval())\n",
    "    print(y.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 999, loss: 0.000\n",
      "X: array([[0, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 1]])\n",
      "pred: array([[0.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [0.]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#给定权重和偏差\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1_1 = tf.constant([[-1.],[-1.]])\n",
    "w1_2 = tf.constant([[1.],[1.]])\n",
    "w2 = tf.constant([[1.],[1.]])\n",
    "\n",
    "b1_1 = tf.constant(1.5, shape=[1])\n",
    "b1_2 = tf.constant(-0.5, shape=[1])\n",
    "b2 = tf.constant(-1.5, shape=[1])\n",
    "\n",
    "h1 = my_st_tf(tf.matmul(x, w1_1) + b1_1)\n",
    "h2 = my_st_tf(tf.matmul(x, w1_2) + b1_2)\n",
    "\n",
    "hidden = tf.concat([h1, h2], 1)\n",
    "out = my_st_tf(tf.matmul(hidden, w2) + b2)\n",
    "loss = tf.reduce_mean(tf.square(out - y))\n",
    "\n",
    "#train = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    loss_ = sess.run(loss, feed_dict={x: X, y: Y})\n",
    "    print(\"step: %d, loss: %.3f\"%(i, loss_))\n",
    "    #for i in range(1000):\n",
    "        #for j in range(4):\n",
    "            #sess.run(train, feed_dict={x: np.expand_dims(X[j], 0), y: np.expand_dims(Y[j], 0)})\n",
    "        #loss_ = sess.run(loss, feed_dict={x: X, y: Y})\n",
    "        #print(\"step: %d, loss: %.3f\"%(i, loss_))\n",
    "    print(\"X: %r\"%X)\n",
    "    print(\"pred: %r\"%sess.run(out, feed_dict={x: X}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 3.348\n",
      "step: 200, loss: 0.000\n",
      "step: 400, loss: 0.000\n",
      "step: 600, loss: 0.000\n",
      "step: 800, loss: 0.000\n",
      "step: 1000, loss: 0.000\n",
      "step: 1200, loss: 0.000\n",
      "step: 1400, loss: 0.000\n",
      "step: 1600, loss: 0.000\n",
      "step: 1800, loss: 0.000\n",
      "step: 2000, loss: 0.000\n",
      "step: 2200, loss: 0.000\n",
      "step: 2400, loss: 0.000\n",
      "step: 2600, loss: 0.000\n",
      "step: 2800, loss: 0.000\n",
      "step: 3000, loss: 0.000\n",
      "step: 3200, loss: 0.000\n",
      "step: 3400, loss: 0.000\n",
      "step: 3600, loss: 0.000\n",
      "step: 3800, loss: 0.000\n",
      "step: 4000, loss: 0.000\n",
      "step: 4200, loss: 0.000\n",
      "step: 4400, loss: 0.000\n",
      "step: 4600, loss: 0.000\n",
      "step: 4800, loss: 0.000\n",
      "X: array([[0, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 1]])\n",
      "pred: array([[ 0.0000000e+00],\n",
      "       [ 1.0000000e+00],\n",
      "       [ 1.0000000e+00],\n",
      "       [-1.1920929e-07]], dtype=float32)\n",
      "b1_1: 0.408 , b1_2: -1.000 , b2: -0.408\n"
     ]
    }
   ],
   "source": [
    "#固定转换矩阵，求截距，relu激活\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1_1 = tf.constant([[1.],[1.]])\n",
    "w1_2 = tf.constant([[1.],[1.]])\n",
    "w2 = tf.constant([[1.],[-2.]])\n",
    "\n",
    "b1_1 = tf.Variable(tf.random_normal([1, 1]))\n",
    "b1_2 = tf.Variable(tf.random_normal([1, 1]))\n",
    "b2 = tf.Variable(tf.random_normal([1, 1]))\n",
    "\n",
    "h1 = tf.nn.relu(tf.matmul(x, w1_1) + b1_1)\n",
    "h2 = tf.nn.relu(tf.matmul(x, w1_2) + b1_2)\n",
    "\n",
    "hidden = tf.concat([h1, h2], 1)\n",
    "out = tf.matmul(hidden, w2) + b2\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(out - y))\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(5000):\n",
    "        if i < 1000:\n",
    "          lr = 0.5\n",
    "        elif i < 3000:\n",
    "          lr = 0.1\n",
    "        else:\n",
    "          lr = 0.01\n",
    "        sess.run(train, feed_dict={x: X, y: Y, learning_rate:lr})\n",
    "        loss_ = sess.run(loss, feed_dict={x: X, y: Y})\n",
    "        if not i % 200:\n",
    "            print(\"step: %d, loss: %.3f\"%(i, loss_))\n",
    "    print(\"X: %r\"%X)\n",
    "    print(\"pred: %r\"%sess.run(out, feed_dict={x: X}))\n",
    "    print(\"b1_1: %.3f , b1_2: %.3f , b2: %.3f\"%(b1_1.eval(),b1_2.eval(),b2.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.731\n",
      "step: 500, loss: 0.531\n",
      "step: 1000, loss: 0.532\n",
      "step: 1500, loss: 0.532\n",
      "step: 2000, loss: 0.532\n",
      "step: 2500, loss: 0.533\n",
      "step: 3000, loss: 0.532\n",
      "step: 3500, loss: 1.132\n",
      "step: 4000, loss: 0.532\n",
      "step: 4500, loss: 1.132\n",
      "X: array([[0, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 1]])\n",
      "out: array([[ 0.5661192],\n",
      "       [ 0.5661192],\n",
      "       [ 0.5661192],\n",
      "       [-1.4338808]], dtype=float32)\n",
      "pred: array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [0.]], dtype=float32)\n",
      "b1_1: 0.082 , b1_2: -1.004 , b2: -0.434\n"
     ]
    }
   ],
   "source": [
    "#固定转换矩阵，求截距，stack激活\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1_1 = tf.constant([[1.],[1.]])\n",
    "w1_2 = tf.constant([[1.],[1.]])\n",
    "w2 = tf.constant([[1.],[-2.]])\n",
    "\n",
    "b1_1 = tf.Variable(tf.random_normal([1, 1]))\n",
    "b1_2 = tf.Variable(tf.random_normal([1, 1]))\n",
    "b2 = tf.Variable(tf.random_normal([1, 1]))\n",
    "\n",
    "h1 = my_st_tf(tf.matmul(x, w1_1) + b1_1)\n",
    "h2 = my_st_tf(tf.matmul(x, w1_2) + b1_2)\n",
    "\n",
    "hidden = tf.concat([h1, h2], 1)\n",
    "out = tf.matmul(hidden, w2) + b2\n",
    "pred = my_st_tf(out)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=out))\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(5000):\n",
    "        if i < 1000:\n",
    "          lr = 0.5\n",
    "        elif i < 3000:\n",
    "          lr = 0.1\n",
    "        else:\n",
    "          lr = 0.01\n",
    "        sess.run(train, feed_dict={x: X, y: Y,learning_rate:lr})\n",
    "        loss_ = sess.run(loss, feed_dict={x: X, y: Y})\n",
    "        if not i % 500:\n",
    "            print(\"step: %d, loss: %.3f\"%(i, loss_))\n",
    "    print(\"X: %r\"%X)\n",
    "    print(\"out: %r\"%sess.run(out, feed_dict={x: X}))\n",
    "    print(\"pred: %r\"%sess.run(pred, feed_dict={x: X}))\n",
    "    print(\"b1_1: %.3f , b1_2: %.3f , b2: %.3f\"%(b1_1.eval(),b1_2.eval(),b2.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.812\n",
      "step: 500, loss: 0.015\n",
      "step: 1000, loss: 0.003\n",
      "step: 1500, loss: 0.001\n",
      "step: 2000, loss: 0.000\n",
      "step: 2500, loss: 0.000\n",
      "step: 3000, loss: 0.000\n",
      "step: 3500, loss: 0.000\n",
      "step: 4000, loss: 0.000\n",
      "step: 4500, loss: 0.000\n",
      "X: array([[0., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 1.]])\n",
      "pred: array([[1.4375511e-06],\n",
      "       [1.0000000e+00],\n",
      "       [9.9999988e-01],\n",
      "       [1.4375511e-06]], dtype=float32)\n",
      "w1:<tf.Variable 'Variable_182:0' shape=(2, 4) dtype=float32_ref>\n",
      "w2:<tf.Variable 'Variable_183:0' shape=(4, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "Y = np.array([[0.], [1.], [1.], [0.]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 4]))\n",
    "w2 = tf.Variable(tf.random_normal([4, 1]))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([4,]))\n",
    "b2 = tf.Variable(tf.random_normal([1,]))\n",
    "\n",
    "hidden = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "\n",
    "out = tf.matmul(hidden, w2) + b2\n",
    "\n",
    "pred = tf.nn.sigmoid(out)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=out))\n",
    "\n",
    "train = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2000):       #2000步收敛\n",
    "        for j in range(4):\n",
    "            sess.run(train, feed_dict={x: np.expand_dims(X[j], 0), y: np.expand_dims(Y[j], 0)})\n",
    "        loss_ = sess.run(loss, feed_dict={x: X, y: Y})\n",
    "        if not i % 500:\n",
    "            print(\"step: %d, loss: %.3f\"%(i, loss_))\n",
    "    print(\"X: %r\"%X)\n",
    "    print(\"pred: %r\"%sess.run(pred, feed_dict={x: X}))\n",
    "    print(\"w1:%r\"%w1)\n",
    "    print(\"w2:%r\"%w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3(tensorflow)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
