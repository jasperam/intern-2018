{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# %load src/XORV2.py\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "   file name: XORV2.py\n",
    "   create time: Sun 16 Jul 2017 07:00:20 AM EDT\n",
    "   author: Jipeng Huang\n",
    "   e-mail: huangjipengnju@gmail.com\n",
    "   github: https://github.com/hjptriplebee\n",
    "   function: improved model can fit XOR\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1_1 = tf.constant([1, 1],shape=[2, 1],dtype=tf.float32)    #设定特解\n",
    "w1_2 = tf.constant([1, 1],shape=[2, 1],dtype=tf.float32)\n",
    "w2 = tf.constant([1, -2],shape=[2, 1],dtype=tf.float32)\n",
    "\n",
    "b1_1 = tf.constant(0.0, shape=[1],dtype=tf.float32)\n",
    "b1_2 = tf.constant(-1.0, shape=[1],dtype=tf.float32)\n",
    "b2 = tf.constant(0.0, shape=[1],dtype=tf.float32)\n",
    "\n",
    "h1 = tf.nn.relu(tf.matmul(x, w1_1) + b1_1)  #用relu激活函数，即整流形函数y=max{x,0}\n",
    "h2 = tf.nn.relu(tf.matmul(x, w1_2) + b1_2)\n",
    "\n",
    "hidden = tf.concat([h1, h2], 1)  #按照第1维连接向量\n",
    "out = tf.matmul(hidden, w2) + b2  \n",
    "\n",
    "loss = tf.reduce_mean(tf.square(out - y))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    loss_ = sess.run(loss, feed_dict={x: X, y: Y})\n",
    "    print(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 2.816\n",
      "step: 50, loss: 0.011\n",
      "step: 100, loss: 0.001\n",
      "step: 150, loss: 0.000\n",
      "step: 200, loss: 0.000\n",
      "step: 250, loss: 0.000\n",
      "step: 300, loss: 0.000\n",
      "step: 350, loss: 0.000\n",
      "step: 400, loss: 0.000\n",
      "step: 450, loss: 0.000\n",
      "step: 500, loss: 0.000\n",
      "step: 550, loss: 0.000\n",
      "step: 600, loss: 0.000\n",
      "step: 650, loss: 0.000\n",
      "step: 700, loss: 0.000\n",
      "step: 750, loss: 0.000\n",
      "step: 800, loss: 0.000\n",
      "step: 850, loss: 0.000\n",
      "step: 900, loss: 0.000\n",
      "step: 950, loss: 0.000\n",
      "X: array([[0, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 1]])\n",
      "pred: array([[  1.90607086e-09],\n",
      "       [  9.99999702e-01],\n",
      "       [  9.99999702e-01],\n",
      "       [  5.97952521e-07]], dtype=float32)\n",
      "b1_1: -0.000, b1_2: -1.000, b2: 0.000, loss:0.00000000000013379663\n"
     ]
    }
   ],
   "source": [
    "# %load src/XORV2.py\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "   file name: XORV2.py\n",
    "   create time: Sun 16 Jul 2017 07:00:20 AM EDT\n",
    "   author: Jipeng Huang\n",
    "   e-mail: huangjipengnju@gmail.com\n",
    "   github: https://github.com/hjptriplebee\n",
    "   function: improved model can fit XOR\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1_1 = tf.constant([1.0, 1.0],shape=[2, 1])\n",
    "w1_2 = tf.constant([1.0, 1.0],shape=[2, 1])\n",
    "w2 = tf.constant([1.0, -2.0],shape=[2, 1])\n",
    "\n",
    "b1_1 = tf.Variable(tf.random_normal([1, 1],seed=1))\n",
    "b1_2 = tf.Variable(tf.random_normal([1, 1],seed=1))\n",
    "b2 = tf.Variable(tf.random_normal([1, 1],seed=1))\n",
    "\n",
    "h1 = tf.nn.relu(tf.matmul(x, w1_1) + b1_1)\n",
    "h2 = tf.nn.relu(tf.matmul(x, w1_2) + b1_2)\n",
    "\n",
    "hidden = tf.concat([h1, h2], 1)\n",
    "out = tf.matmul(hidden, w2) + b2\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(out - y)) #均方误差 还可用交叉熵\n",
    "\n",
    "train = tf.train.AdamOptimizer(0.01).minimize(loss) #自适应矩估计优化\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        for j in range(4):\n",
    "            sess.run(train, feed_dict={x: np.expand_dims(X[j], 0), y: np.expand_dims(Y[j], 0)})\n",
    "        loss_ = sess.run(loss, feed_dict={x: X, y: Y})\n",
    "        if not i % 50:\n",
    "            print(\"step: %d, loss: %.3f\"%(i, loss_))\n",
    "    print(\"X: %r\"%X)\n",
    "    print(\"pred: %r\"%sess.run(out, feed_dict={x: X}))\n",
    "    print(\"b1_1: %.3f, b1_2: %.3f, b2: %.3f, loss:%.20f\"%(b1_1.eval(),b1_2.eval(),b2.eval(),loss_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 3.139\n",
      "step: 1, loss: 2.505\n",
      "step: 2, loss: 1.978\n",
      "step: 3, loss: 1.554\n",
      "step: 4, loss: 1.221\n",
      "step: 5, loss: 0.966\n",
      "step: 6, loss: 0.775\n",
      "step: 7, loss: 0.635\n",
      "step: 8, loss: 0.534\n",
      "step: 9, loss: 0.463\n",
      "step: 10, loss: 0.413\n",
      "step: 11, loss: 0.379\n",
      "step: 12, loss: 0.355\n",
      "step: 13, loss: 0.339\n",
      "step: 14, loss: 0.327\n",
      "step: 15, loss: 0.319\n",
      "step: 16, loss: 0.312\n",
      "step: 17, loss: 0.307\n",
      "step: 18, loss: 0.302\n",
      "step: 19, loss: 0.298\n",
      "step: 20, loss: 0.294\n",
      "step: 21, loss: 0.291\n",
      "step: 22, loss: 0.289\n",
      "step: 23, loss: 0.287\n",
      "step: 24, loss: 0.284\n",
      "step: 25, loss: 0.282\n",
      "step: 26, loss: 0.279\n",
      "step: 27, loss: 0.277\n",
      "step: 28, loss: 0.274\n",
      "step: 29, loss: 0.272\n",
      "step: 30, loss: 0.270\n",
      "step: 31, loss: 0.267\n",
      "step: 32, loss: 0.265\n",
      "step: 33, loss: 0.263\n",
      "step: 34, loss: 0.261\n",
      "step: 35, loss: 0.259\n",
      "step: 36, loss: 0.257\n",
      "step: 37, loss: 0.255\n",
      "step: 38, loss: 0.253\n",
      "step: 39, loss: 0.251\n",
      "step: 40, loss: 0.249\n",
      "step: 41, loss: 0.248\n",
      "step: 42, loss: 0.246\n",
      "step: 43, loss: 0.244\n",
      "step: 44, loss: 0.242\n",
      "step: 45, loss: 0.240\n",
      "step: 46, loss: 0.239\n",
      "step: 47, loss: 0.237\n",
      "step: 48, loss: 0.235\n",
      "step: 49, loss: 0.234\n",
      "step: 50, loss: 0.232\n",
      "step: 51, loss: 0.231\n",
      "step: 52, loss: 0.229\n",
      "step: 53, loss: 0.228\n",
      "step: 54, loss: 0.226\n",
      "step: 55, loss: 0.225\n",
      "step: 56, loss: 0.223\n",
      "step: 57, loss: 0.222\n",
      "step: 58, loss: 0.220\n",
      "step: 59, loss: 0.219\n",
      "step: 60, loss: 0.217\n",
      "step: 61, loss: 0.216\n",
      "step: 62, loss: 0.214\n",
      "step: 63, loss: 0.213\n",
      "step: 64, loss: 0.212\n",
      "step: 65, loss: 0.210\n",
      "step: 66, loss: 0.209\n",
      "step: 67, loss: 0.207\n",
      "step: 68, loss: 0.206\n",
      "step: 69, loss: 0.204\n",
      "step: 70, loss: 0.203\n",
      "step: 71, loss: 0.202\n",
      "step: 72, loss: 0.200\n",
      "step: 73, loss: 0.199\n",
      "step: 74, loss: 0.198\n",
      "step: 75, loss: 0.196\n",
      "step: 76, loss: 0.194\n",
      "step: 77, loss: 0.193\n",
      "step: 78, loss: 0.191\n",
      "step: 79, loss: 0.190\n",
      "step: 80, loss: 0.189\n",
      "step: 81, loss: 0.187\n",
      "step: 82, loss: 0.186\n",
      "step: 83, loss: 0.184\n",
      "step: 84, loss: 0.183\n",
      "step: 85, loss: 0.181\n",
      "step: 86, loss: 0.180\n",
      "step: 87, loss: 0.178\n",
      "step: 88, loss: 0.177\n",
      "step: 89, loss: 0.175\n",
      "step: 90, loss: 0.174\n",
      "step: 91, loss: 0.172\n",
      "step: 92, loss: 0.170\n",
      "step: 93, loss: 0.169\n",
      "step: 94, loss: 0.167\n",
      "step: 95, loss: 0.166\n",
      "step: 96, loss: 0.164\n",
      "step: 97, loss: 0.163\n",
      "step: 98, loss: 0.161\n",
      "step: 99, loss: 0.160\n",
      "step: 100, loss: 0.158\n",
      "step: 101, loss: 0.156\n",
      "step: 102, loss: 0.155\n",
      "step: 103, loss: 0.153\n",
      "step: 104, loss: 0.152\n",
      "step: 105, loss: 0.150\n",
      "step: 106, loss: 0.148\n",
      "step: 107, loss: 0.146\n",
      "step: 108, loss: 0.145\n",
      "step: 109, loss: 0.143\n",
      "step: 110, loss: 0.142\n",
      "step: 111, loss: 0.141\n",
      "step: 112, loss: 0.139\n",
      "step: 113, loss: 0.137\n",
      "step: 114, loss: 0.135\n",
      "step: 115, loss: 0.133\n",
      "step: 116, loss: 0.132\n",
      "step: 117, loss: 0.131\n",
      "step: 118, loss: 0.129\n",
      "step: 119, loss: 0.127\n",
      "step: 120, loss: 0.125\n",
      "step: 121, loss: 0.124\n",
      "step: 122, loss: 0.122\n",
      "step: 123, loss: 0.121\n",
      "step: 124, loss: 0.119\n",
      "step: 125, loss: 0.118\n",
      "step: 126, loss: 0.116\n",
      "step: 127, loss: 0.115\n",
      "step: 128, loss: 0.113\n",
      "step: 129, loss: 0.111\n",
      "step: 130, loss: 0.110\n",
      "step: 131, loss: 0.108\n",
      "step: 132, loss: 0.107\n",
      "step: 133, loss: 0.105\n",
      "step: 134, loss: 0.104\n",
      "step: 135, loss: 0.102\n",
      "step: 136, loss: 0.101\n",
      "step: 137, loss: 0.099\n",
      "step: 138, loss: 0.098\n",
      "step: 139, loss: 0.097\n",
      "step: 140, loss: 0.095\n",
      "step: 141, loss: 0.093\n",
      "step: 142, loss: 0.092\n",
      "step: 143, loss: 0.091\n",
      "step: 144, loss: 0.089\n",
      "step: 145, loss: 0.088\n",
      "step: 146, loss: 0.086\n",
      "step: 147, loss: 0.085\n",
      "step: 148, loss: 0.084\n",
      "step: 149, loss: 0.082\n",
      "step: 150, loss: 0.081\n",
      "step: 151, loss: 0.080\n",
      "step: 152, loss: 0.078\n",
      "step: 153, loss: 0.077\n",
      "step: 154, loss: 0.076\n",
      "step: 155, loss: 0.074\n",
      "step: 156, loss: 0.073\n",
      "step: 157, loss: 0.071\n",
      "step: 158, loss: 0.069\n",
      "step: 159, loss: 0.068\n",
      "step: 160, loss: 0.066\n",
      "step: 161, loss: 0.064\n",
      "step: 162, loss: 0.062\n",
      "step: 163, loss: 0.061\n",
      "step: 164, loss: 0.059\n",
      "step: 165, loss: 0.057\n",
      "step: 166, loss: 0.056\n",
      "step: 167, loss: 0.054\n",
      "step: 168, loss: 0.053\n",
      "step: 169, loss: 0.051\n",
      "step: 170, loss: 0.050\n",
      "step: 171, loss: 0.049\n",
      "step: 172, loss: 0.047\n",
      "step: 173, loss: 0.046\n",
      "step: 174, loss: 0.045\n",
      "step: 175, loss: 0.043\n",
      "step: 176, loss: 0.042\n",
      "step: 177, loss: 0.041\n",
      "step: 178, loss: 0.040\n",
      "step: 179, loss: 0.038\n",
      "step: 180, loss: 0.037\n",
      "step: 181, loss: 0.036\n",
      "step: 182, loss: 0.035\n",
      "step: 183, loss: 0.034\n",
      "step: 184, loss: 0.033\n",
      "step: 185, loss: 0.032\n",
      "step: 186, loss: 0.031\n",
      "step: 187, loss: 0.030\n",
      "step: 188, loss: 0.029\n",
      "step: 189, loss: 0.028\n",
      "step: 190, loss: 0.027\n",
      "step: 191, loss: 0.026\n",
      "step: 192, loss: 0.025\n",
      "step: 193, loss: 0.024\n",
      "step: 194, loss: 0.024\n",
      "step: 195, loss: 0.023\n",
      "step: 196, loss: 0.022\n",
      "step: 197, loss: 0.021\n",
      "step: 198, loss: 0.020\n",
      "step: 199, loss: 0.020\n",
      "step: 200, loss: 0.019\n",
      "step: 201, loss: 0.018\n",
      "step: 202, loss: 0.018\n",
      "step: 203, loss: 0.017\n",
      "step: 204, loss: 0.016\n",
      "step: 205, loss: 0.016\n",
      "step: 206, loss: 0.015\n",
      "step: 207, loss: 0.014\n",
      "step: 208, loss: 0.014\n",
      "step: 209, loss: 0.013\n",
      "step: 210, loss: 0.013\n",
      "step: 211, loss: 0.012\n",
      "step: 212, loss: 0.012\n",
      "step: 213, loss: 0.011\n",
      "step: 214, loss: 0.011\n",
      "step: 215, loss: 0.010\n",
      "step: 216, loss: 0.010\n",
      "step: 217, loss: 0.010\n",
      "step: 218, loss: 0.009\n",
      "step: 219, loss: 0.009\n",
      "step: 220, loss: 0.008\n",
      "step: 221, loss: 0.008\n",
      "step: 222, loss: 0.008\n",
      "step: 223, loss: 0.007\n",
      "step: 224, loss: 0.007\n",
      "step: 225, loss: 0.007\n",
      "step: 226, loss: 0.006\n",
      "step: 227, loss: 0.006\n",
      "step: 228, loss: 0.006\n",
      "step: 229, loss: 0.006\n",
      "step: 230, loss: 0.005\n",
      "step: 231, loss: 0.005\n",
      "step: 232, loss: 0.005\n",
      "step: 233, loss: 0.005\n",
      "step: 234, loss: 0.004\n",
      "step: 235, loss: 0.004\n",
      "step: 236, loss: 0.004\n",
      "step: 237, loss: 0.004\n",
      "step: 238, loss: 0.004\n",
      "step: 239, loss: 0.003\n",
      "step: 240, loss: 0.003\n",
      "step: 241, loss: 0.003\n",
      "step: 242, loss: 0.003\n",
      "step: 243, loss: 0.003\n",
      "step: 244, loss: 0.003\n",
      "step: 245, loss: 0.003\n",
      "step: 246, loss: 0.002\n",
      "step: 247, loss: 0.002\n",
      "step: 248, loss: 0.002\n",
      "step: 249, loss: 0.002\n",
      "step: 250, loss: 0.002\n",
      "step: 251, loss: 0.002\n",
      "step: 252, loss: 0.002\n",
      "step: 253, loss: 0.002\n",
      "step: 254, loss: 0.002\n",
      "step: 255, loss: 0.001\n",
      "step: 256, loss: 0.001\n",
      "step: 257, loss: 0.001\n",
      "step: 258, loss: 0.001\n",
      "step: 259, loss: 0.001\n",
      "step: 260, loss: 0.001\n",
      "step: 261, loss: 0.001\n",
      "step: 262, loss: 0.001\n",
      "step: 263, loss: 0.001\n",
      "step: 264, loss: 0.001\n",
      "step: 265, loss: 0.001\n",
      "step: 266, loss: 0.001\n",
      "step: 267, loss: 0.001\n",
      "step: 268, loss: 0.001\n",
      "step: 269, loss: 0.001\n",
      "step: 270, loss: 0.001\n",
      "step: 271, loss: 0.001\n",
      "step: 272, loss: 0.001\n",
      "step: 273, loss: 0.001\n",
      "step: 274, loss: 0.000\n",
      "step: 275, loss: 0.000\n",
      "step: 276, loss: 0.000\n",
      "step: 277, loss: 0.000\n",
      "step: 278, loss: 0.000\n",
      "step: 279, loss: 0.000\n",
      "step: 280, loss: 0.000\n",
      "step: 281, loss: 0.000\n",
      "step: 282, loss: 0.000\n",
      "step: 283, loss: 0.000\n",
      "step: 284, loss: 0.000\n",
      "step: 285, loss: 0.000\n",
      "step: 286, loss: 0.000\n",
      "step: 287, loss: 0.000\n",
      "step: 288, loss: 0.000\n",
      "step: 289, loss: 0.000\n",
      "step: 290, loss: 0.000\n",
      "step: 291, loss: 0.000\n",
      "step: 292, loss: 0.000\n",
      "step: 293, loss: 0.000\n",
      "step: 294, loss: 0.000\n",
      "step: 295, loss: 0.000\n",
      "step: 296, loss: 0.000\n",
      "step: 297, loss: 0.000\n",
      "step: 298, loss: 0.000\n",
      "step: 299, loss: 0.000\n",
      "step: 300, loss: 0.000\n",
      "step: 301, loss: 0.000\n",
      "step: 302, loss: 0.000\n",
      "step: 303, loss: 0.000\n",
      "step: 304, loss: 0.000\n",
      "step: 305, loss: 0.000\n",
      "step: 306, loss: 0.000\n",
      "step: 307, loss: 0.000\n",
      "step: 308, loss: 0.000\n",
      "step: 309, loss: 0.000\n",
      "step: 310, loss: 0.000\n",
      "step: 311, loss: 0.000\n",
      "step: 312, loss: 0.000\n",
      "step: 313, loss: 0.000\n",
      "step: 314, loss: 0.000\n",
      "step: 315, loss: 0.000\n",
      "step: 316, loss: 0.000\n",
      "step: 317, loss: 0.000\n",
      "step: 318, loss: 0.000\n",
      "step: 319, loss: 0.000\n",
      "step: 320, loss: 0.000\n",
      "step: 321, loss: 0.000\n",
      "step: 322, loss: 0.000\n",
      "step: 323, loss: 0.000\n",
      "step: 324, loss: 0.000\n",
      "step: 325, loss: 0.000\n",
      "step: 326, loss: 0.000\n",
      "step: 327, loss: 0.000\n",
      "step: 328, loss: 0.000\n",
      "step: 329, loss: 0.000\n",
      "step: 330, loss: 0.000\n",
      "step: 331, loss: 0.000\n",
      "step: 332, loss: 0.000\n",
      "step: 333, loss: 0.000\n",
      "step: 334, loss: 0.000\n",
      "step: 335, loss: 0.000\n",
      "step: 336, loss: 0.000\n",
      "step: 337, loss: 0.000\n",
      "step: 338, loss: 0.000\n",
      "step: 339, loss: 0.000\n",
      "step: 340, loss: 0.000\n",
      "step: 341, loss: 0.000\n",
      "step: 342, loss: 0.000\n",
      "step: 343, loss: 0.000\n",
      "step: 344, loss: 0.000\n",
      "step: 345, loss: 0.000\n",
      "step: 346, loss: 0.000\n",
      "step: 347, loss: 0.000\n",
      "step: 348, loss: 0.000\n",
      "step: 349, loss: 0.000\n",
      "step: 350, loss: 0.000\n",
      "step: 351, loss: 0.000\n",
      "step: 352, loss: 0.000\n",
      "step: 353, loss: 0.000\n",
      "step: 354, loss: 0.000\n",
      "step: 355, loss: 0.000\n",
      "step: 356, loss: 0.000\n",
      "step: 357, loss: 0.000\n",
      "step: 358, loss: 0.000\n",
      "step: 359, loss: 0.000\n",
      "step: 360, loss: 0.000\n",
      "step: 361, loss: 0.000\n",
      "step: 362, loss: 0.000\n",
      "step: 363, loss: 0.000\n",
      "step: 364, loss: 0.000\n",
      "step: 365, loss: 0.000\n",
      "step: 366, loss: 0.000\n",
      "step: 367, loss: 0.000\n",
      "step: 368, loss: 0.000\n",
      "step: 369, loss: 0.000\n",
      "step: 370, loss: 0.000\n",
      "step: 371, loss: 0.000\n",
      "step: 372, loss: 0.000\n",
      "step: 373, loss: 0.000\n",
      "step: 374, loss: 0.000\n",
      "step: 375, loss: 0.000\n",
      "step: 376, loss: 0.000\n",
      "step: 377, loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 378, loss: 0.000\n",
      "step: 379, loss: 0.000\n",
      "step: 380, loss: 0.000\n",
      "step: 381, loss: 0.000\n",
      "step: 382, loss: 0.000\n",
      "step: 383, loss: 0.000\n",
      "step: 384, loss: 0.000\n",
      "step: 385, loss: 0.000\n",
      "step: 386, loss: 0.000\n",
      "step: 387, loss: 0.000\n",
      "step: 388, loss: 0.000\n",
      "step: 389, loss: 0.000\n",
      "step: 390, loss: 0.000\n",
      "step: 391, loss: 0.000\n",
      "step: 392, loss: 0.000\n",
      "step: 393, loss: 0.000\n",
      "step: 394, loss: 0.000\n",
      "step: 395, loss: 0.000\n",
      "step: 396, loss: 0.000\n",
      "step: 397, loss: 0.000\n",
      "step: 398, loss: 0.000\n",
      "step: 399, loss: 0.000\n",
      "step: 400, loss: 0.000\n",
      "step: 401, loss: 0.000\n",
      "step: 402, loss: 0.000\n",
      "step: 403, loss: 0.000\n",
      "step: 404, loss: 0.000\n",
      "step: 405, loss: 0.000\n",
      "step: 406, loss: 0.000\n",
      "step: 407, loss: 0.000\n",
      "step: 408, loss: 0.000\n",
      "step: 409, loss: 0.000\n",
      "step: 410, loss: 0.000\n",
      "step: 411, loss: 0.000\n",
      "step: 412, loss: 0.000\n",
      "step: 413, loss: 0.000\n",
      "step: 414, loss: 0.000\n",
      "step: 415, loss: 0.000\n",
      "step: 416, loss: 0.000\n",
      "step: 417, loss: 0.000\n",
      "step: 418, loss: 0.000\n",
      "step: 419, loss: 0.000\n",
      "step: 420, loss: 0.000\n",
      "step: 421, loss: 0.000\n",
      "step: 422, loss: 0.000\n",
      "step: 423, loss: 0.000\n",
      "step: 424, loss: 0.000\n",
      "step: 425, loss: 0.000\n",
      "step: 426, loss: 0.000\n",
      "step: 427, loss: 0.000\n",
      "step: 428, loss: 0.000\n",
      "step: 429, loss: 0.000\n",
      "step: 430, loss: 0.000\n",
      "step: 431, loss: 0.000\n",
      "step: 432, loss: 0.000\n",
      "step: 433, loss: 0.000\n",
      "step: 434, loss: 0.000\n",
      "step: 435, loss: 0.000\n",
      "step: 436, loss: 0.000\n",
      "step: 437, loss: 0.000\n",
      "step: 438, loss: 0.000\n",
      "step: 439, loss: 0.000\n",
      "step: 440, loss: 0.000\n",
      "step: 441, loss: 0.000\n",
      "step: 442, loss: 0.000\n",
      "step: 443, loss: 0.000\n",
      "step: 444, loss: 0.000\n",
      "step: 445, loss: 0.000\n",
      "step: 446, loss: 0.000\n",
      "step: 447, loss: 0.000\n",
      "step: 448, loss: 0.000\n",
      "step: 449, loss: 0.000\n",
      "step: 450, loss: 0.000\n",
      "step: 451, loss: 0.000\n",
      "step: 452, loss: 0.000\n",
      "step: 453, loss: 0.000\n",
      "step: 454, loss: 0.000\n",
      "step: 455, loss: 0.000\n",
      "step: 456, loss: 0.000\n",
      "step: 457, loss: 0.000\n",
      "step: 458, loss: 0.000\n",
      "step: 459, loss: 0.000\n",
      "step: 460, loss: 0.000\n",
      "step: 461, loss: 0.000\n",
      "step: 462, loss: 0.000\n",
      "step: 463, loss: 0.000\n",
      "step: 464, loss: 0.000\n",
      "step: 465, loss: 0.000\n",
      "step: 466, loss: 0.000\n",
      "step: 467, loss: 0.000\n",
      "step: 468, loss: 0.000\n",
      "step: 469, loss: 0.000\n",
      "step: 470, loss: 0.000\n",
      "step: 471, loss: 0.000\n",
      "step: 472, loss: 0.000\n",
      "step: 473, loss: 0.000\n",
      "step: 474, loss: 0.000\n",
      "step: 475, loss: 0.000\n",
      "step: 476, loss: 0.000\n",
      "step: 477, loss: 0.000\n",
      "step: 478, loss: 0.000\n",
      "step: 479, loss: 0.000\n",
      "step: 480, loss: 0.000\n",
      "step: 481, loss: 0.000\n",
      "step: 482, loss: 0.000\n",
      "step: 483, loss: 0.000\n",
      "step: 484, loss: 0.000\n",
      "step: 485, loss: 0.000\n",
      "step: 486, loss: 0.000\n",
      "step: 487, loss: 0.000\n",
      "step: 488, loss: 0.000\n",
      "step: 489, loss: 0.000\n",
      "step: 490, loss: 0.000\n",
      "step: 491, loss: 0.000\n",
      "step: 492, loss: 0.000\n",
      "step: 493, loss: 0.000\n",
      "step: 494, loss: 0.000\n",
      "step: 495, loss: 0.000\n",
      "step: 496, loss: 0.000\n",
      "step: 497, loss: 0.000\n",
      "step: 498, loss: 0.000\n",
      "step: 499, loss: 0.000\n",
      "step: 500, loss: 0.000\n",
      "step: 501, loss: 0.000\n",
      "step: 502, loss: 0.000\n",
      "step: 503, loss: 0.000\n",
      "step: 504, loss: 0.000\n",
      "step: 505, loss: 0.000\n",
      "step: 506, loss: 0.000\n",
      "step: 507, loss: 0.000\n",
      "step: 508, loss: 0.000\n",
      "step: 509, loss: 0.000\n",
      "step: 510, loss: 0.000\n",
      "step: 511, loss: 0.000\n",
      "step: 512, loss: 0.000\n",
      "step: 513, loss: 0.000\n",
      "step: 514, loss: 0.000\n",
      "step: 515, loss: 0.000\n",
      "step: 516, loss: 0.000\n",
      "step: 517, loss: 0.000\n",
      "step: 518, loss: 0.000\n",
      "step: 519, loss: 0.000\n",
      "step: 520, loss: 0.000\n",
      "step: 521, loss: 0.000\n",
      "step: 522, loss: 0.000\n",
      "step: 523, loss: 0.000\n",
      "step: 524, loss: 0.000\n",
      "step: 525, loss: 0.000\n",
      "step: 526, loss: 0.000\n",
      "step: 527, loss: 0.000\n",
      "step: 528, loss: 0.000\n",
      "step: 529, loss: 0.000\n",
      "step: 530, loss: 0.000\n",
      "step: 531, loss: 0.000\n",
      "step: 532, loss: 0.000\n",
      "step: 533, loss: 0.000\n",
      "step: 534, loss: 0.000\n",
      "step: 535, loss: 0.000\n",
      "step: 536, loss: 0.000\n",
      "step: 537, loss: 0.000\n",
      "step: 538, loss: 0.000\n",
      "step: 539, loss: 0.000\n",
      "step: 540, loss: 0.000\n",
      "step: 541, loss: 0.000\n",
      "step: 542, loss: 0.000\n",
      "step: 543, loss: 0.000\n",
      "step: 544, loss: 0.000\n",
      "step: 545, loss: 0.000\n",
      "step: 546, loss: 0.000\n",
      "step: 547, loss: 0.000\n",
      "step: 548, loss: 0.000\n",
      "step: 549, loss: 0.000\n",
      "step: 550, loss: 0.000\n",
      "step: 551, loss: 0.000\n",
      "step: 552, loss: 0.000\n",
      "step: 553, loss: 0.000\n",
      "step: 554, loss: 0.000\n",
      "step: 555, loss: 0.000\n",
      "step: 556, loss: 0.000\n",
      "step: 557, loss: 0.000\n",
      "step: 558, loss: 0.000\n",
      "step: 559, loss: 0.000\n",
      "step: 560, loss: 0.000\n",
      "step: 561, loss: 0.000\n",
      "step: 562, loss: 0.000\n",
      "step: 563, loss: 0.000\n",
      "step: 564, loss: 0.000\n",
      "step: 565, loss: 0.000\n",
      "step: 566, loss: 0.000\n",
      "step: 567, loss: 0.000\n",
      "step: 568, loss: 0.000\n",
      "step: 569, loss: 0.000\n",
      "step: 570, loss: 0.000\n",
      "step: 571, loss: 0.000\n",
      "step: 572, loss: 0.000\n",
      "step: 573, loss: 0.000\n",
      "step: 574, loss: 0.000\n",
      "step: 575, loss: 0.000\n",
      "step: 576, loss: 0.000\n",
      "step: 577, loss: 0.000\n",
      "step: 578, loss: 0.000\n",
      "step: 579, loss: 0.000\n",
      "step: 580, loss: 0.000\n",
      "step: 581, loss: 0.000\n",
      "step: 582, loss: 0.000\n",
      "step: 583, loss: 0.000\n",
      "step: 584, loss: 0.000\n",
      "step: 585, loss: 0.000\n",
      "step: 586, loss: 0.000\n",
      "step: 587, loss: 0.000\n",
      "step: 588, loss: 0.000\n",
      "step: 589, loss: 0.000\n",
      "step: 590, loss: 0.000\n",
      "step: 591, loss: 0.000\n",
      "step: 592, loss: 0.000\n",
      "step: 593, loss: 0.000\n",
      "step: 594, loss: 0.000\n",
      "step: 595, loss: 0.000\n",
      "step: 596, loss: 0.000\n",
      "step: 597, loss: 0.000\n",
      "step: 598, loss: 0.000\n",
      "step: 599, loss: 0.000\n",
      "step: 600, loss: 0.000\n",
      "step: 601, loss: 0.000\n",
      "step: 602, loss: 0.000\n",
      "step: 603, loss: 0.000\n",
      "step: 604, loss: 0.000\n",
      "step: 605, loss: 0.000\n",
      "step: 606, loss: 0.000\n",
      "step: 607, loss: 0.000\n",
      "step: 608, loss: 0.000\n",
      "step: 609, loss: 0.000\n",
      "step: 610, loss: 0.000\n",
      "step: 611, loss: 0.000\n",
      "step: 612, loss: 0.000\n",
      "step: 613, loss: 0.000\n",
      "step: 614, loss: 0.000\n",
      "step: 615, loss: 0.000\n",
      "step: 616, loss: 0.000\n",
      "step: 617, loss: 0.000\n",
      "step: 618, loss: 0.000\n",
      "step: 619, loss: 0.000\n",
      "step: 620, loss: 0.000\n",
      "step: 621, loss: 0.000\n",
      "step: 622, loss: 0.000\n",
      "step: 623, loss: 0.000\n",
      "step: 624, loss: 0.000\n",
      "step: 625, loss: 0.000\n",
      "step: 626, loss: 0.000\n",
      "step: 627, loss: 0.000\n",
      "step: 628, loss: 0.000\n",
      "step: 629, loss: 0.000\n",
      "step: 630, loss: 0.000\n",
      "step: 631, loss: 0.000\n",
      "step: 632, loss: 0.000\n",
      "step: 633, loss: 0.000\n",
      "step: 634, loss: 0.000\n",
      "step: 635, loss: 0.000\n",
      "step: 636, loss: 0.000\n",
      "step: 637, loss: 0.000\n",
      "step: 638, loss: 0.000\n",
      "step: 639, loss: 0.000\n",
      "step: 640, loss: 0.000\n",
      "step: 641, loss: 0.000\n",
      "step: 642, loss: 0.000\n",
      "step: 643, loss: 0.000\n",
      "step: 644, loss: 0.000\n",
      "step: 645, loss: 0.000\n",
      "step: 646, loss: 0.000\n",
      "step: 647, loss: 0.000\n",
      "step: 648, loss: 0.000\n",
      "step: 649, loss: 0.000\n",
      "step: 650, loss: 0.000\n",
      "step: 651, loss: 0.000\n",
      "step: 652, loss: 0.000\n",
      "step: 653, loss: 0.000\n",
      "step: 654, loss: 0.000\n",
      "step: 655, loss: 0.000\n",
      "step: 656, loss: 0.000\n",
      "step: 657, loss: 0.000\n",
      "step: 658, loss: 0.000\n",
      "step: 659, loss: 0.000\n",
      "step: 660, loss: 0.000\n",
      "step: 661, loss: 0.000\n",
      "step: 662, loss: 0.000\n",
      "step: 663, loss: 0.000\n",
      "step: 664, loss: 0.000\n",
      "step: 665, loss: 0.000\n",
      "step: 666, loss: 0.000\n",
      "step: 667, loss: 0.000\n",
      "step: 668, loss: 0.000\n",
      "step: 669, loss: 0.000\n",
      "step: 670, loss: 0.000\n",
      "step: 671, loss: 0.000\n",
      "step: 672, loss: 0.000\n",
      "step: 673, loss: 0.000\n",
      "step: 674, loss: 0.000\n",
      "step: 675, loss: 0.000\n",
      "step: 676, loss: 0.000\n",
      "step: 677, loss: 0.000\n",
      "step: 678, loss: 0.000\n",
      "step: 679, loss: 0.000\n",
      "step: 680, loss: 0.000\n",
      "step: 681, loss: 0.000\n",
      "step: 682, loss: 0.000\n",
      "step: 683, loss: 0.000\n",
      "step: 684, loss: 0.000\n",
      "step: 685, loss: 0.000\n",
      "step: 686, loss: 0.000\n",
      "step: 687, loss: 0.000\n",
      "step: 688, loss: 0.000\n",
      "step: 689, loss: 0.000\n",
      "step: 690, loss: 0.000\n",
      "step: 691, loss: 0.000\n",
      "step: 692, loss: 0.000\n",
      "step: 693, loss: 0.000\n",
      "step: 694, loss: 0.000\n",
      "step: 695, loss: 0.000\n",
      "step: 696, loss: 0.000\n",
      "step: 697, loss: 0.000\n",
      "step: 698, loss: 0.000\n",
      "step: 699, loss: 0.000\n",
      "step: 700, loss: 0.000\n",
      "step: 701, loss: 0.000\n",
      "step: 702, loss: 0.000\n",
      "step: 703, loss: 0.000\n",
      "step: 704, loss: 0.000\n",
      "step: 705, loss: 0.000\n",
      "step: 706, loss: 0.000\n",
      "step: 707, loss: 0.000\n",
      "step: 708, loss: 0.000\n",
      "step: 709, loss: 0.000\n",
      "step: 710, loss: 0.000\n",
      "step: 711, loss: 0.000\n",
      "step: 712, loss: 0.000\n",
      "step: 713, loss: 0.000\n",
      "step: 714, loss: 0.000\n",
      "step: 715, loss: 0.000\n",
      "step: 716, loss: 0.000\n",
      "step: 717, loss: 0.000\n",
      "step: 718, loss: 0.000\n",
      "step: 719, loss: 0.000\n",
      "step: 720, loss: 0.000\n",
      "step: 721, loss: 0.000\n",
      "step: 722, loss: 0.000\n",
      "step: 723, loss: 0.000\n",
      "step: 724, loss: 0.000\n",
      "step: 725, loss: 0.000\n",
      "step: 726, loss: 0.000\n",
      "step: 727, loss: 0.000\n",
      "step: 728, loss: 0.000\n",
      "step: 729, loss: 0.000\n",
      "step: 730, loss: 0.000\n",
      "step: 731, loss: 0.000\n",
      "step: 732, loss: 0.000\n",
      "step: 733, loss: 0.000\n",
      "step: 734, loss: 0.000\n",
      "step: 735, loss: 0.000\n",
      "step: 736, loss: 0.000\n",
      "step: 737, loss: 0.000\n",
      "step: 738, loss: 0.000\n",
      "step: 739, loss: 0.000\n",
      "step: 740, loss: 0.000\n",
      "step: 741, loss: 0.000\n",
      "step: 742, loss: 0.000\n",
      "step: 743, loss: 0.000\n",
      "step: 744, loss: 0.000\n",
      "step: 745, loss: 0.000\n",
      "step: 746, loss: 0.000\n",
      "step: 747, loss: 0.000\n",
      "step: 748, loss: 0.000\n",
      "step: 749, loss: 0.000\n",
      "step: 750, loss: 0.000\n",
      "step: 751, loss: 0.000\n",
      "step: 752, loss: 0.000\n",
      "step: 753, loss: 0.000\n",
      "step: 754, loss: 0.000\n",
      "step: 755, loss: 0.000\n",
      "step: 756, loss: 0.000\n",
      "step: 757, loss: 0.000\n",
      "step: 758, loss: 0.000\n",
      "step: 759, loss: 0.000\n",
      "step: 760, loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 761, loss: 0.000\n",
      "step: 762, loss: 0.000\n",
      "step: 763, loss: 0.000\n",
      "step: 764, loss: 0.000\n",
      "step: 765, loss: 0.000\n",
      "step: 766, loss: 0.000\n",
      "step: 767, loss: 0.000\n",
      "step: 768, loss: 0.000\n",
      "step: 769, loss: 0.000\n",
      "step: 770, loss: 0.000\n",
      "step: 771, loss: 0.000\n",
      "step: 772, loss: 0.000\n",
      "step: 773, loss: 0.000\n",
      "step: 774, loss: 0.000\n",
      "step: 775, loss: 0.000\n",
      "step: 776, loss: 0.000\n",
      "step: 777, loss: 0.000\n",
      "step: 778, loss: 0.000\n",
      "step: 779, loss: 0.000\n",
      "step: 780, loss: 0.000\n",
      "step: 781, loss: 0.000\n",
      "step: 782, loss: 0.000\n",
      "step: 783, loss: 0.000\n",
      "step: 784, loss: 0.000\n",
      "step: 785, loss: 0.000\n",
      "step: 786, loss: 0.000\n",
      "step: 787, loss: 0.000\n",
      "step: 788, loss: 0.000\n",
      "step: 789, loss: 0.000\n",
      "step: 790, loss: 0.000\n",
      "step: 791, loss: 0.000\n",
      "step: 792, loss: 0.000\n",
      "step: 793, loss: 0.000\n",
      "step: 794, loss: 0.000\n",
      "step: 795, loss: 0.000\n",
      "step: 796, loss: 0.000\n",
      "step: 797, loss: 0.000\n",
      "step: 798, loss: 0.000\n",
      "step: 799, loss: 0.000\n",
      "step: 800, loss: 0.000\n",
      "step: 801, loss: 0.000\n",
      "step: 802, loss: 0.000\n",
      "step: 803, loss: 0.000\n",
      "step: 804, loss: 0.000\n",
      "step: 805, loss: 0.000\n",
      "step: 806, loss: 0.000\n",
      "step: 807, loss: 0.000\n",
      "step: 808, loss: 0.000\n",
      "step: 809, loss: 0.000\n",
      "step: 810, loss: 0.000\n",
      "step: 811, loss: 0.000\n",
      "step: 812, loss: 0.000\n",
      "step: 813, loss: 0.000\n",
      "step: 814, loss: 0.000\n",
      "step: 815, loss: 0.000\n",
      "step: 816, loss: 0.000\n",
      "step: 817, loss: 0.000\n",
      "step: 818, loss: 0.000\n",
      "step: 819, loss: 0.000\n",
      "step: 820, loss: 0.000\n",
      "step: 821, loss: 0.000\n",
      "step: 822, loss: 0.000\n",
      "step: 823, loss: 0.000\n",
      "step: 824, loss: 0.000\n",
      "step: 825, loss: 0.000\n",
      "step: 826, loss: 0.000\n",
      "step: 827, loss: 0.000\n",
      "step: 828, loss: 0.000\n",
      "step: 829, loss: 0.000\n",
      "step: 830, loss: 0.000\n",
      "step: 831, loss: 0.000\n",
      "step: 832, loss: 0.000\n",
      "step: 833, loss: 0.000\n",
      "step: 834, loss: 0.000\n",
      "step: 835, loss: 0.000\n",
      "step: 836, loss: 0.000\n",
      "step: 837, loss: 0.000\n",
      "step: 838, loss: 0.000\n",
      "step: 839, loss: 0.000\n",
      "step: 840, loss: 0.000\n",
      "step: 841, loss: 0.000\n",
      "step: 842, loss: 0.000\n",
      "step: 843, loss: 0.000\n",
      "step: 844, loss: 0.000\n",
      "step: 845, loss: 0.000\n",
      "step: 846, loss: 0.000\n",
      "step: 847, loss: 0.000\n",
      "step: 848, loss: 0.000\n",
      "step: 849, loss: 0.000\n",
      "step: 850, loss: 0.000\n",
      "step: 851, loss: 0.000\n",
      "step: 852, loss: 0.000\n",
      "step: 853, loss: 0.000\n",
      "step: 854, loss: 0.000\n",
      "step: 855, loss: 0.000\n",
      "step: 856, loss: 0.000\n",
      "step: 857, loss: 0.000\n",
      "step: 858, loss: 0.000\n",
      "step: 859, loss: 0.000\n",
      "step: 860, loss: 0.000\n",
      "step: 861, loss: 0.000\n",
      "step: 862, loss: 0.000\n",
      "step: 863, loss: 0.000\n",
      "step: 864, loss: 0.000\n",
      "step: 865, loss: 0.000\n",
      "step: 866, loss: 0.000\n",
      "step: 867, loss: 0.000\n",
      "step: 868, loss: 0.000\n",
      "step: 869, loss: 0.000\n",
      "step: 870, loss: 0.000\n",
      "step: 871, loss: 0.000\n",
      "step: 872, loss: 0.000\n",
      "step: 873, loss: 0.000\n",
      "step: 874, loss: 0.000\n",
      "step: 875, loss: 0.000\n",
      "step: 876, loss: 0.000\n",
      "step: 877, loss: 0.000\n",
      "step: 878, loss: 0.000\n",
      "step: 879, loss: 0.000\n",
      "step: 880, loss: 0.000\n",
      "step: 881, loss: 0.000\n",
      "step: 882, loss: 0.000\n",
      "step: 883, loss: 0.000\n",
      "step: 884, loss: 0.000\n",
      "step: 885, loss: 0.000\n",
      "step: 886, loss: 0.000\n",
      "step: 887, loss: 0.000\n",
      "step: 888, loss: 0.000\n",
      "step: 889, loss: 0.000\n",
      "step: 890, loss: 0.000\n",
      "step: 891, loss: 0.000\n",
      "step: 892, loss: 0.000\n",
      "step: 893, loss: 0.000\n",
      "step: 894, loss: 0.000\n",
      "step: 895, loss: 0.000\n",
      "step: 896, loss: 0.000\n",
      "step: 897, loss: 0.000\n",
      "step: 898, loss: 0.000\n",
      "step: 899, loss: 0.000\n",
      "step: 900, loss: 0.000\n",
      "step: 901, loss: 0.000\n",
      "step: 902, loss: 0.000\n",
      "step: 903, loss: 0.000\n",
      "step: 904, loss: 0.000\n",
      "step: 905, loss: 0.000\n",
      "step: 906, loss: 0.000\n",
      "step: 907, loss: 0.000\n",
      "step: 908, loss: 0.000\n",
      "step: 909, loss: 0.000\n",
      "step: 910, loss: 0.000\n",
      "step: 911, loss: 0.000\n",
      "step: 912, loss: 0.000\n",
      "step: 913, loss: 0.000\n",
      "step: 914, loss: 0.000\n",
      "step: 915, loss: 0.000\n",
      "step: 916, loss: 0.000\n",
      "step: 917, loss: 0.000\n",
      "step: 918, loss: 0.000\n",
      "step: 919, loss: 0.000\n",
      "step: 920, loss: 0.000\n",
      "step: 921, loss: 0.000\n",
      "step: 922, loss: 0.000\n",
      "step: 923, loss: 0.000\n",
      "step: 924, loss: 0.000\n",
      "step: 925, loss: 0.000\n",
      "step: 926, loss: 0.000\n",
      "step: 927, loss: 0.000\n",
      "step: 928, loss: 0.000\n",
      "step: 929, loss: 0.000\n",
      "step: 930, loss: 0.000\n",
      "step: 931, loss: 0.000\n",
      "step: 932, loss: 0.000\n",
      "step: 933, loss: 0.000\n",
      "step: 934, loss: 0.000\n",
      "step: 935, loss: 0.000\n",
      "step: 936, loss: 0.000\n",
      "step: 937, loss: 0.000\n",
      "step: 938, loss: 0.000\n",
      "step: 939, loss: 0.000\n",
      "step: 940, loss: 0.000\n",
      "step: 941, loss: 0.000\n",
      "step: 942, loss: 0.000\n",
      "step: 943, loss: 0.000\n",
      "step: 944, loss: 0.000\n",
      "step: 945, loss: 0.000\n",
      "step: 946, loss: 0.000\n",
      "step: 947, loss: 0.000\n",
      "step: 948, loss: 0.000\n",
      "step: 949, loss: 0.000\n",
      "step: 950, loss: 0.000\n",
      "step: 951, loss: 0.000\n",
      "step: 952, loss: 0.000\n",
      "step: 953, loss: 0.000\n",
      "step: 954, loss: 0.000\n",
      "step: 955, loss: 0.000\n",
      "step: 956, loss: 0.000\n",
      "step: 957, loss: 0.000\n",
      "step: 958, loss: 0.000\n",
      "step: 959, loss: 0.000\n",
      "step: 960, loss: 0.000\n",
      "step: 961, loss: 0.000\n",
      "step: 962, loss: 0.000\n",
      "step: 963, loss: 0.000\n",
      "step: 964, loss: 0.000\n",
      "step: 965, loss: 0.000\n",
      "step: 966, loss: 0.000\n",
      "step: 967, loss: 0.000\n",
      "step: 968, loss: 0.000\n",
      "step: 969, loss: 0.000\n",
      "step: 970, loss: 0.000\n",
      "step: 971, loss: 0.000\n",
      "step: 972, loss: 0.000\n",
      "step: 973, loss: 0.000\n",
      "step: 974, loss: 0.000\n",
      "step: 975, loss: 0.000\n",
      "step: 976, loss: 0.000\n",
      "step: 977, loss: 0.000\n",
      "step: 978, loss: 0.000\n",
      "step: 979, loss: 0.000\n",
      "step: 980, loss: 0.000\n",
      "step: 981, loss: 0.000\n",
      "step: 982, loss: 0.000\n",
      "step: 983, loss: 0.000\n",
      "step: 984, loss: 0.000\n",
      "step: 985, loss: 0.000\n",
      "step: 986, loss: 0.000\n",
      "step: 987, loss: 0.000\n",
      "step: 988, loss: 0.000\n",
      "step: 989, loss: 0.000\n",
      "step: 990, loss: 0.000\n",
      "step: 991, loss: 0.000\n",
      "step: 992, loss: 0.000\n",
      "step: 993, loss: 0.000\n",
      "step: 994, loss: 0.000\n",
      "step: 995, loss: 0.000\n",
      "step: 996, loss: 0.000\n",
      "step: 997, loss: 0.000\n",
      "step: 998, loss: 0.000\n",
      "step: 999, loss: 0.000\n",
      "X: array([[0, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 1]])\n",
      "pred: array([[  3.12924385e-07],\n",
      "       [  1.00000000e+00],\n",
      "       [  9.99999344e-01],\n",
      "       [  3.12924385e-07]], dtype=float32)\n",
      "w1_1: [0.592,-0.592] w1_2: [-1.087,1.199] w2: [1.689,1.103]\n",
      "b1_1: 0.117, b1_2: -0.112, b2: -0.198\n"
     ]
    }
   ],
   "source": [
    "# %load src/XORV2.py\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "   file name: XORV2.py\n",
    "   create time: Sun 16 Jul 2017 07:00:20 AM EDT\n",
    "   author: Jipeng Huang\n",
    "   e-mail: huangjipengnju@gmail.com\n",
    "   github: https://github.com/hjptriplebee\n",
    "   function: improved model can fit XOR\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1_1 = tf.Variable(tf.random_normal([2, 1]))  #全部放开为优化变量\n",
    "w1_2 = tf.Variable(tf.random_normal([2, 1]))\n",
    "w2 = tf.Variable(tf.random_normal([2, 1]))\n",
    "\n",
    "b1_1 = tf.Variable(tf.random_normal([1]))\n",
    "b1_2 = tf.Variable(tf.random_normal([1]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "h1 = tf.nn.relu(tf.matmul(x, w1_1) + b1_1)\n",
    "h2 = tf.nn.relu(tf.matmul(x, w1_2) + b1_2)\n",
    "\n",
    "hidden = tf.concat([h1, h2], 1)\n",
    "out = tf.matmul(hidden, w2) + b2\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(out - y))\n",
    "#loss = tf.nn.softmax_cross_entropy_with_logits(out,y)\n",
    "\n",
    "train = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        for j in range(4):\n",
    "            sess.run(train, feed_dict={x: np.expand_dims(X[j], 0), y: np.expand_dims(Y[j], 0)})\n",
    "        loss_ = sess.run(loss, feed_dict={x: X, y: Y})\n",
    "        print(\"step: %d, loss: %.3f\"%(i, loss_))\n",
    "    print(\"X: %r\"%X)\n",
    "    print(\"pred: %r\"%sess.run(out, feed_dict={x: X}))\n",
    "    print(\"w1_1: [%.3f,%.3f] w1_2: [%.3f,%.3f] w2: [%.3f,%.3f]\"%(w1_1[0].eval(),w1_1[1].eval(),w1_2[0].eval(),w1_2[1].eval(),w2[0].eval(),w2[1].eval()))\n",
    "    print(\"b1_1: %.3f, b1_2: %.3f, b2: %.3f\"%(b1_1.eval(),b1_2.eval(),b2.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
