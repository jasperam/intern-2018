{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络\n",
    "## 1.架构介绍\n",
    "全连接神经网络的每一层都接收了上一层所有节点的信息，并且单层的神经元之间相互独立不共享任何参数。而卷积神经网络中使用了卷积运算，相比而言有三个重要特征，稀疏交互、参数共享和等变表示。\n",
    "### 卷积运算\n",
    "### 图像识别应用\n",
    "MNIST的例子，输入层为\\[32\\*32\\*3\\]，使用全连接网络会有参数过多、计算代价高的问题，因此使用卷积网络，输出层为\\[1\\*1\\*10\\]对应10类得分。\n",
    "![Full VS CNN](image\\net.png)\n",
    "\"A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.\"\n",
    "## 2.卷积网络层\n",
    "结构：Input -- Conv -- Relu -- Pool -- FullC \n",
    "<br />1.Input:\\[32\\*32\\*3\\] （RGB）\n",
    "<br />2.Conv:点乘权重和所连接的小区域，若选择12个filters则得到\\[32\\*32\\*12\\]。\n",
    "<br />3.Relu:激活层，维度保持不变。\n",
    "<br />4.Pool:对width和height缩减采样，得到\\[16\\*16\\*12\\]。\n",
    "<br />5.FullC:计算每类得分，得到\\[1\\*1\\*10\\]\n",
    "<br />Conv和FullC即包含激活部分也包含参数部分，参数由优化算法得到，Pool和Relu仅为一层固定函数。\n",
    "![Car](image\\car.png)\n",
    "### 卷积层\n",
    "卷积层参数由一系列可学习的过滤器（filters）组成，比如一个\\[5\\*5\\*3\\]的过滤器（depth保证和输入相同，width和height小于输入）。将单个过滤器遍历与原图相应位置点乘，得到2维的激活图，网络对该激活图学习，可得到如第一层的边缘信息、某些颜色的斑点和更高层的抽象图案等特征。假如有12个过滤器，每个对应一个2维的激活图，最后合在一起的维度\\[32\\*32\\*12\\]。\n",
    "#### 稀疏连接(Local Connectivity)\n",
    "过滤器的大小又称为接收域（如5×5），在图片识别中再次强调宽和高小于原始输入，深度等于原始输入也就是3。故卷积层的神经元的权重大小即为5×5×3=75（再加1个偏差）。\n",
    "![Arch](image\\arch.png)\n",
    "#### 空间排列(Spatial arrangement)\n",
    "输出卷积（volume）的排列涉及三个超参数，深度（depth）、步幅（stride）和补零法（zero_padding）\n",
    "<br />1.depth:过滤器的个数，每个有不同的含义，比如那原始图片作为输入层，就可以有边缘识别、颜色判断等过滤器。depth column指的是不同神经元对应同一块输入区域。\n",
    "<br />2.stride:遍历的步幅长度，影响输出卷积的大小。\n",
    "<br />3.zero-padding:边缘补0，控制输出大小\n",
    "![size](image\\size.png)\n",
    "#### 参数共享(Parameter Sharing)\n",
    "对于size为\\[55\\*55\\*96\\]的卷（volume），有96个depth slices（\\[55\\*55\\]），参数共享指的是同一slice的神经元（neuron）使用相同的权重和偏差，固整体参数个数为96\\*11\\*11\\*3=34848,+96biases。后馈计算时，会按slice整体更新参数。由于参数共享的机制，每一层的计算实际上是神经元的权重和输入卷的卷积（convolution），过滤器（filter）也称为卷积中的核函数（kernel）。\n",
    "<br />有一些情况参数共享不适用，如果原图的识别受位置影响，如脸部识别问题，这时需要采用局部连接层方法（Locally-Connected Layer）\n",
    "![sum](image\\sum.png)\n",
    "### 池化层\n",
    "此层的作用是逐渐减少空间大小以减少参数和相应计算复杂度，同时也可以控制过拟合的问题。\n",
    "![pool1](image\\pool1.png)\n",
    "![pool2](image\\pool2.png)\n",
    "一些情况池化层并不好用，可以通过增大步幅的方式代替池化减少size的方法。丢弃池化层在一些问题里是很重要的，如变化的自编码器、生成对抗网络（GANS)等。\n",
    "### 全连接层\n",
    "全连接层与卷积层唯一的区别在于输入是否为局部的，任一卷积层可用一个全连接层达到相同的运算，而全连接层可以视作过滤器大小与输入相等、步幅为1的卷积层。\n",
    "\n",
    "## 3.卷积网络架构\n",
    "### 分层模式\n",
    "一般的卷积网络结构：INPUT -> \\[[CONV -> RELU]\\*N -> POOL?\\]\\*M -> \\[FC -> RELU\\]\\*K -> FC\n",
    "<br />一般而言，倾向于用一串小的过滤卷积层代替一个大的过滤卷积层。对比三个3×3的卷积层和一个7×7的卷积层，对局部信息的输入范围是类似的，但前者每两层之间包含了非线性联系，并且参数要少于后者，缺点在于前者再后馈计算时需要更多的内存空间。\n",
    "<br /> 近年，传统的线性列表结构受到了挑战，未来更复杂更不同的连接结构更被倾向于使用，我们实战时可以参考类似于ImageNet等已有的网络结构，根据自己的数据进行调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history\n"
     ]
    }
   ],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3(tensorflow)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
